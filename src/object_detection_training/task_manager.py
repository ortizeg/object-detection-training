#!/usr/bin/env python
"""
Task Manager CLI - Main entrypoint for running training tasks.

This module provides a Hydra-based CLI for running object detection
training tasks with full configuration management.

Usage:
    python -m object_detection_training.task_manager
    python -m object_detection_training.task_manager model=rfdetr_small
    python -m object_detection_training.task_manager trainer.max_epochs=50
"""

import sys

import hydra
import numpy as np
import onnx.helper
from hydra.core.hydra_config import HydraConfig
from loguru import logger
from omegaconf import DictConfig, OmegaConf

from object_detection_training.utils.hydra import (
    instantiate_callbacks,
    instantiate_datamodule,
    instantiate_loggers,
    instantiate_model,
    instantiate_trainer,
)

# Monkey-patch onnx.helper.float32_to_bfloat16 if it doesn't exist
# This is required because onnx-graphsurgeon (v0.5.8) expects this function,
# but it was removed in recent onnx versions (>=1.16.0).
if not hasattr(onnx.helper, "float32_to_bfloat16"):
    logger.warning("Monkey-patching onnx.helper.float32_to_bfloat16 for compatibility")

    def float32_to_bfloat16(x):
        # bfloat16 is the top 16 bits of float32
        y = np.ascontiguousarray(x).view(np.uint32)
        return (y >> 16).astype(np.uint16)

    onnx.helper.float32_to_bfloat16 = float32_to_bfloat16

# Configure loguru to show all pytorch internal logs
logger.remove()
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | "
    "<level>{level: <8}</level> | "
    "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - "
    "<level>{message}</level>",
    level="DEBUG",
)


def setup_logging():
    """Configure logging to show all PyTorch internal logs."""
    import logging
    import warnings

    # Enable all warnings
    warnings.filterwarnings("default")

    # Set all loggers to DEBUG
    logging.getLogger().setLevel(logging.DEBUG)
    logging.getLogger("pytorch_lightning").setLevel(logging.DEBUG)
    logging.getLogger("lightning").setLevel(logging.DEBUG)
    logging.getLogger("torch").setLevel(logging.DEBUG)

    # Make sure PyTorch prints all internal info
    import torch

    torch.set_printoptions(profile="full")


@hydra.main(version_base=None, config_path="../../conf", config_name="train")
def main(cfg: DictConfig) -> None:
    """
    Main entrypoint for the task manager.

    Args:
        cfg: Hydra configuration dictionary.
    """
    setup_logging()

    logger.info("=" * 60)
    logger.info("Object Detection Training Framework")
    logger.info("=" * 60)
    logger.info(f"Configuration:\n{OmegaConf.to_yaml(cfg)}")

    # Get the output directory generated by Hydra
    output_dir = HydraConfig.get().runtime.output_dir
    logger.info(f"Output directory: {output_dir}")

    # Instantiate components
    # Note: Instantiate datamodule first to get num_classes for model
    logger.info("Instantiating datamodule...")
    datamodule = instantiate_datamodule(cfg.data)

    # Auto-detect num_classes from data if model doesn't specify it
    num_classes = datamodule.num_classes
    logger.info(f"Auto-detected num_classes={num_classes} from dataset")

    logger.info("Instantiating model...")
    model = instantiate_model(cfg.model, num_classes=num_classes)

    logger.info("Instantiating callbacks...")
    callbacks = instantiate_callbacks(cfg.callbacks)

    logger.info("Instantiating loggers...")
    loggers = instantiate_loggers(cfg.get("logging"))

    logger.info("Instantiating trainer...")
    trainer = instantiate_trainer(cfg.trainer, callbacks=callbacks, loggers=loggers)

    # Instantiate and run the task with all components
    logger.info("Instantiating task...")
    task = hydra.utils.instantiate(
        cfg.task,
        output_dir=output_dir,
        model=model,
        data=datamodule,
        trainer=trainer,
        callbacks=callbacks,
        loggers=loggers,
    )

    logger.info(f"Task type: {type(task).__name__}")
    result = task()

    logger.info("=" * 60)
    logger.info("Task completed!")
    if result:
        logger.info(f"Results: {result}")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()
