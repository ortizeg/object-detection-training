from copy import copy

import hydra
import numpy as np
import onnx.helper
from hydra.core.hydra_config import HydraConfig
from loguru import logger
from omegaconf import DictConfig, OmegaConf

import object_detection_training.models.rfdetr_wrappers as _  # noqa: F401

# Monkey-patch onnx.helper.float32_to_bfloat16 if it doesn't exist
# This is required because onnx-graphsurgeon (v0.5.8) expects this function,
# but it was removed in recent onnx versions (>=1.16.0).
if not hasattr(onnx.helper, "float32_to_bfloat16"):
    logger.warning("Monkey-patching onnx.helper.float32_to_bfloat16 for compatibility")

    def float32_to_bfloat16(x):
        # bfloat16 is the top 16 bits of float32
        y = np.ascontiguousarray(x).view(np.uint32)
        return (y >> 16).astype(np.uint16)

    onnx.helper.float32_to_bfloat16 = float32_to_bfloat16


@hydra.main(version_base=None, config_path="../../conf", config_name="train")
def train(cfg: DictConfig) -> None:
    """
    Training entrypoint using Hydra for configuration.
    """
    logger.info(f"Training configuration:\n{OmegaConf.to_yaml(cfg)}")

    # Get the output directory generated by Hydra
    if cfg.training.output_dir is None:
        output_dir = HydraConfig.get().runtime.output_dir
    else:
        output_dir = cfg.training.output_dir

    logger.info(f"Output directory: {output_dir}")

    logger.info("Instantiating model...")
    # Instantiate the model from configuration
    model = hydra.utils.instantiate(cfg.models)
    logger.info(f"Model instantiated: {type(model).__name__}")

    logger.info("Starting training process...")
    model.train(
        dataset_dir=cfg.training.dataset_dir,
        epochs=cfg.training.epochs,
        num_workers=cfg.training.num_workers,
        lr=cfg.training.lr,
        lr_encoder=cfg.training.lr_encoder,
        use_ema=cfg.training.use_ema,
        ema_decay=cfg.training.ema_decay,
        device=cfg.training.device,
        world_size=cfg.training.world_size,
        dist_url=cfg.training.dist_url,
        early_stopping=cfg.training.early_stopping,
        early_stopping_patience=cfg.training.early_stopping_patience,
        batch_size=cfg.training.batch_size,
        grad_accum_steps=cfg.training.grad_accum_steps,
        amp=cfg.training.amp,
        tensorboard=cfg.training.tensorboard,
        output_dir=output_dir,
    )
    logger.info("Training completed successfully.")

    # Export latest model
    logger.info("Exporting latest model to ONNX...")
    model.export(
        output_dir=output_dir,
        infer_dir=None,
        simplify=True,
        backbone_only=False,
        opset_version=17,
        verbose=True,
        force=False,
        shape=None,
        batch_size=1,
    )
    del model

    logger.info("Exporting best model to ONNX...")
    best_regular_cfg = copy(cfg.models)
    best_regular_cfg.pretrain_weights = f"{output_dir}/checkpoint_best_regular.pth"
    model_best = hydra.utils.instantiate(best_regular_cfg)
    model_best.export(f"{output_dir}/checkpoint_best_regular.onnx")
    del model_best

    # Attempt to load and export best EMA model
    if cfg.training.use_ema:
        logger.info("Exporting best EMA model to ONNX...")
        best_ema_cfg = copy(cfg.models)
        best_ema_cfg.pretrain_weights = f"{output_dir}/checkpoint_best_ema.pth"
        model_best_ema = hydra.utils.instantiate(best_ema_cfg)
        model_best_ema.export(f"{output_dir}/checkpoint_best_ema.onnx")
        del model_best_ema
        logger.info("Best EMA model exported successfully.")


if __name__ == "__main__":
    train()
