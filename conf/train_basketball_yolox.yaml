# Basketball YOLOX Cloud Training Configuration
# ================================================
# Tuned for NVIDIA L4 (24 GB) / T4 (16 GB) GPUs.
#
# Usage (GCP / cloud):
#   python -m object_detection_training.task_manager \
#       --config-name train_basketball_yolox \
#       data.train_path=/path/to/train \
#       data.val_path=/path/to/val
#
# For T4 (16 GB) reduce batch size:
#   ... data.batch_size=16 trainer.accumulate_grad_batches=4

defaults:
  - task: train
  - models: yolox_s
  - data: data_basketball
  - trainer: trainer_yolox
  - callbacks: default
  - logging: null
  - _self_

# Global Parameters
input_height: 640
input_width: 640
log_level: INFO

# Override model for basketball (10 classes)
# Fine-tuning from COCO pretrained weights — use lower LR than training-from-scratch
models:
  num_classes: 10
  learning_rate: 0.001
  weight_decay: 2e-4
  warmup_epochs: 5
  freeze_backbone_epochs: 5
  l1_loss_epoch: 30
  iou_loss_type: giou
  input_height: ${input_height}
  input_width: ${input_width}

# Data — L4 fits batch_size=32 at 640x640 for yolox_s
# YOLOX transforms: no box normalization (pixel xyxy throughout),
# no image normalization (raw 0-255), ColorJitter + RandomErasing augmentation
# Mosaic/MixUp disabled — too aggressive for fine-tuning on small dataset
data:
  batch_size: 32
  num_workers: 8
  yolox_transforms: true
  mosaic: false
  multi_scale: false

# Trainer — cloud GPU settings
# Extended training with L1 loss schedule for fine-tuning
trainer:
  max_epochs: 150
  precision: 16-mixed
  gradient_clip_val: 5.0
  accumulate_grad_batches: 2
  val_check_interval: 1.0

# Callback overrides for fine-tuning
callbacks:
  early_stopping:
    patience: 25
  ema:
    decay: 0.9998
    warmup_steps: 2000

# Hydra configuration
hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/yolox_s_basketball_${now:%H-%M-%S}
  sweep:
    dir: outputs/multirun/${now:%Y-%m-%d}/yolox_s_basketball_${now:%H-%M-%S}
    subdir: ${hydra.job.num}
